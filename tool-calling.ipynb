{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01bed5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:30:19,635 - INFO - Model initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('langgraph_workflow.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# init model from ollama\n",
    "model = init_chat_model(\n",
    "    \"ollama:gemma3:12b-it-qat\",\n",
    ")\n",
    "\n",
    "logger.info(\"Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8d62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str):\n",
    "    \"\"\"Returns weather info for a location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather.\n",
    "\n",
    "    Example:\n",
    "        >>> get_weather(\"San Francisco\")\n",
    "    \"\"\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "# Available tools dictionary\n",
    "TOOLS = {\n",
    "    \"get_weather\": get_weather\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4938bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:30:19,644 - INFO - Tool description created:\n",
      "def get_weather(location: str):\n",
      "\"\"\"Returns weather info for a location.\n",
      "\n",
      "    Args:\n",
      "        location (str): The location to get the weather for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string describing the weather.\n",
      "\n",
      "    Example:\n",
      "        >>> get_weather(\"San Francisco\")\n",
      "    \n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "def create_tool_description(tools: dict):\n",
    "    \"\"\"Creates a string description of available tools that we can pass to the model.\n",
    "    \n",
    "    Includes name, description and usage information for each tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each tool, return its docstring\n",
    "    tool_descriptions = []\n",
    "    for tool_name, tool_func in tools.items():\n",
    "        signature = inspect.signature(tool_func)\n",
    "        docstring = tool_func.__doc__\n",
    "        tool_descriptions.append(f\"def {tool_name}{signature}:\\n\\\"\\\"\\\"{docstring}\\n\\\"\\\"\\\"\")\n",
    "    return \"\\n\".join(tool_descriptions)\n",
    "\n",
    "# test that it works\n",
    "tool_description = create_tool_description(TOOLS)\n",
    "logger.info(f\"Tool description created:\\n{tool_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "683b353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful conversational AI assistant.\n",
      "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
      "The python methods described below are imported and available, you can only use defined methods.\n",
      "ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
      "The generated code should be readable and efficient.\n",
      "\n",
      "For questions that don't require any specific tools, just respond normally without tool calls.\n",
      "\n",
      "The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
      "When using a ```tool_call``` think step by step why and how it should be used.\n",
      "\n",
      "The following Python methods are available:\n",
      "\n",
      "```python\n",
      "def get_weather(location: str):\n",
      "\"\"\"Returns weather info for a location.\n",
      "\n",
      "    Args:\n",
      "        location (str): The location to get the weather for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string describing the weather.\n",
      "\n",
      "    Example:\n",
      "        >>> get_weather(\"San Francisco\")\n",
      "    \n",
      "\"\"\"\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure you do not include any \".\" in the prompt - you will get errors during the function call!S\n",
    "instruction_prompt = f'''You are a helpful conversational AI assistant.\n",
    "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
    "The python methods described below are imported and available, you can only use defined methods.\n",
    "ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
    "The generated code should be readable and efficient.\n",
    "\n",
    "For questions that don't require any specific tools, just respond normally without tool calls.\n",
    "\n",
    "The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
    "When using a ```tool_call``` think step by step why and how it should be used.\n",
    "\n",
    "The following Python methods are available:\n",
    "\n",
    "```python\n",
    "{tool_description}\n",
    "```\n",
    "'''\n",
    "\n",
    "print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c48d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(text):\n",
    "    \"\"\"Extract tool calls from model output using regex parsing.\"\"\"\n",
    "    pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        code = match.group(1).strip()\n",
    "        try:\n",
    "            # Execute the tool call safely\n",
    "            result = eval(code, {\"__builtins__\": {}}, TOOLS)\n",
    "            return f'```tool_output\\n{result}\\n```'\n",
    "        except Exception as e:\n",
    "            return f'```tool_output\\nError: {str(e)}\\n```'\n",
    "    return None\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    logger.info(f\"[ROUTER] Checking last message for tool code...\")\n",
    "    logger.info(f\"[ROUTER] Last message content preview: {str(last_message.content)[:100]}...\")\n",
    "    \n",
    "    # Check if the last message contains tool code\n",
    "    if hasattr(last_message, 'content') and '```tool_code' in str(last_message.content):\n",
    "        logger.info(\"[ROUTER] Tool code detected - routing to tools\")\n",
    "        return \"tools\"\n",
    "    \n",
    "    logger.info(\"[ROUTER] No tool code detected - routing to respond\")\n",
    "    return \"respond\"\n",
    "\n",
    "def should_retry(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message contains a tool error\n",
    "    if hasattr(last_message, 'content') and \"Tool execution failed:\" in str(last_message.content):\n",
    "        logger.info(\"[RETRY] Tool error detected - allowing retry\")\n",
    "        return \"think\"\n",
    "    \n",
    "    logger.info(\"[RETRY] No error detected - proceeding to respond\")\n",
    "    return \"respond\"\n",
    "\n",
    "def think(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    logger.info(f\"[THINK] Processing {len(messages)} messages\")\n",
    "    \n",
    "    # For the first message, create a tool-aware prompt\n",
    "    if len(messages) == 1:\n",
    "        user_message = messages[0].content\n",
    "        logger.info(f\"[THINK] First message - creating tool-aware prompt for: {user_message}\")\n",
    "        \n",
    "        # Create system message with tool instructions\n",
    "        tool_definitions = []\n",
    "        for name, func in TOOLS.items():\n",
    "            tool_definitions.append(f\"def {name}({func.__code__.co_varnames[0]}: str) -> str:\\n    \\\"\\\"\\\"{func.__doc__}\\\"\\\"\\\"\")\n",
    "        \n",
    "        system_prompt = instruction_prompt\n",
    "        \n",
    "        response = model.invoke([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ])\n",
    "    else:\n",
    "        logger.info(\"[THINK] Continuing conversation with existing context\")\n",
    "        response = model.invoke(messages)\n",
    "    \n",
    "    logger.info(f\"[THINK] Model response: {response.content[:200]}...\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def execute_tools(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    logger.info(\"[TOOLS] Executing tool calls...\")\n",
    "    logger.info(f\"[TOOLS] Last message content: {last_message.content}\")\n",
    "    \n",
    "    # Extract and execute tool calls\n",
    "    tool_output = extract_tool_calls(last_message.content)\n",
    "    \n",
    "    if tool_output:\n",
    "        logger.info(f\"[TOOLS] Tool execution result: {tool_output}\")\n",
    "        # Extract just the result from tool_output\n",
    "        result_match = re.search(r'```tool_output\\n(.*?)\\n```', tool_output, re.DOTALL)\n",
    "        if result_match:\n",
    "            clean_result = result_match.group(1).strip()\n",
    "            logger.info(f\"[TOOLS] Clean result: {clean_result}\")\n",
    "            \n",
    "            # Check if there was an error and provide helpful feedback\n",
    "            if \"Error:\" in clean_result:\n",
    "                from langchain_core.messages import AIMessage\n",
    "                error_feedback = f\"\"\"Tool execution failed: {clean_result}\n",
    "\n",
    "If you tried to call a function, remember to use the exact function names:\n",
    "- get_weather(\"location\") ✓\n",
    "- NOT weather_tool.get_weather(\"location\") ✗\n",
    "\n",
    "Available functions: {', '.join(TOOLS.keys())}\"\"\"\n",
    "                response = AIMessage(content=error_feedback)\n",
    "                logger.warning(f\"[TOOLS] Tool error occurred: {clean_result}\")\n",
    "            else:\n",
    "                from langchain_core.messages import AIMessage\n",
    "                response = AIMessage(content=f\"Tool result: {clean_result}\")\n",
    "                logger.info(\"[TOOLS] Tool executed successfully\")\n",
    "            \n",
    "            return {\"messages\": [response]}\n",
    "    \n",
    "    logger.warning(\"[TOOLS] No tool output generated\")\n",
    "    return {\"messages\": []}\n",
    "\n",
    "def respond(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    logger.info(f\"[RESPOND] Generating final response from {len(messages)} messages\")\n",
    "    \n",
    "    # Generate a clean response for the user based on the conversation\n",
    "    system_prompt = \"\"\"Based on the conversation and any tool results, provide a clear, helpful response to the user. \n",
    "    Do not include any tool code or internal reasoning. Just give a direct, conversational answer.\n",
    "    If there are tool results, incorporate them naturally into your response.\"\"\"\n",
    "    \n",
    "    # Add the system prompt and get response\n",
    "    conversation_with_system = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    response = model.invoke(conversation_with_system)\n",
    "    \n",
    "    logger.info(f\"[RESPOND] Final response: {response.content}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023c9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph setup \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"think\", think)\n",
    "builder.add_node(\"tools\", execute_tools)\n",
    "builder.add_node(\"respond\", respond)\n",
    "\n",
    "builder.add_edge(START, \"think\")\n",
    "builder.add_conditional_edges(\"think\", should_continue, [\"tools\", \"respond\"])\n",
    "builder.add_conditional_edges(\"tools\", should_retry, [\"think\", \"respond\"])\n",
    "builder.add_edge(\"respond\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils \n",
    "def print_conversation(result):\n",
    "    print(\"=== CONVERSATION FLOW ===\")\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"\\n--- Message {i+1} ---\")\n",
    "        print(f\"Type: {type(message).__name__}\")\n",
    "        print(f\"Content: {message.content}\")\n",
    "        \n",
    "        # Check if this message contains a tool call\n",
    "        if '```tool_code' in str(message.content):\n",
    "            print(\"🔧 TOOL CALL DETECTED\")\n",
    "            tool_output = extract_tool_calls(message.content)\n",
    "            if tool_output:\n",
    "                print(f\"Tool Result: {tool_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f59c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:31:10,295 - INFO - [THINK] Processing 2 messages\n",
      "2025-07-06 10:31:10,296 - INFO - [THINK] Continuing conversation with existing context\n",
      "2025-07-06 10:31:10,777 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 10:31:11,957 - INFO - [THINK] Model response: It's nice to meet you, Barry! My name is Tim. I'm a helpful assistant. 😊\n",
      "...\n",
      "2025-07-06 10:31:11,958 - INFO - [ROUTER] Checking last message for tool code...\n",
      "2025-07-06 10:31:11,959 - INFO - [ROUTER] Last message content preview: It's nice to meet you, Barry! My name is Tim. I'm a helpful assistant. 😊\n",
      "...\n",
      "2025-07-06 10:31:11,960 - INFO - [ROUTER] No tool code detected - routing to respond\n",
      "2025-07-06 10:31:11,961 - INFO - [RESPOND] Generating final response from 3 messages\n",
      "2025-07-06 10:31:12,697 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 10:31:13,292 - INFO - [RESPOND] Final response: What can I do for you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is your name? I am barry!\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: It's nice to meet you, Barry! My name is Tim. I'm a helpful assistant. 😊\n",
      "\n",
      "\n",
      "--- Message 4 ---\n",
      "Type: AIMessage\n",
      "Content: What can I do for you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#excecute graph - test that chat history works\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is your name? I am barry!\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5fb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:32:07,824 - INFO - [THINK] Processing 2 messages\n",
      "2025-07-06 10:32:07,826 - INFO - [THINK] Continuing conversation with existing context\n",
      "2025-07-06 10:32:08,539 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 10:32:13,432 - INFO - [THINK] Model response: Okay! Let me check the weather for you.\n",
      "\n",
      "Currently in San Francisco, it's **62°F (17°C)** and **mostly cloudy**. The wind is blowing from the west at 8 mph.\n",
      "\n",
      "Here's a more detailed breakdown:\n",
      "\n",
      "*   **H...\n",
      "2025-07-06 10:32:13,434 - INFO - [ROUTER] Checking last message for tool code...\n",
      "2025-07-06 10:32:13,434 - INFO - [ROUTER] Last message content preview: Okay! Let me check the weather for you.\n",
      "\n",
      "Currently in San Francisco, it's **62°F (17°C)** and **most...\n",
      "2025-07-06 10:32:13,434 - INFO - [ROUTER] No tool code detected - routing to respond\n",
      "2025-07-06 10:32:13,436 - INFO - [RESPOND] Generating final response from 3 messages\n",
      "2025-07-06 10:32:14,452 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 10:32:14,453 - INFO - [RESPOND] Final response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is the weather in San Francisco?\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Okay! Let me check the weather for you.\n",
      "\n",
      "Currently in San Francisco, it's **62°F (17°C)** and **mostly cloudy**. The wind is blowing from the west at 8 mph.\n",
      "\n",
      "Here's a more detailed breakdown:\n",
      "\n",
      "*   **Humidity:** 75%\n",
      "*   **Visibility:** 10 miles\n",
      "*   **Sunrise:** 7:07 AM\n",
      "*   **Sunset:** 6:19 PM\n",
      "\n",
      "Would you like any more details, like the forecast for tomorrow or the weekend?\n",
      "\n",
      "\n",
      "--- Message 4 ---\n",
      "Type: AIMessage\n",
      "Content: \n"
     ]
    }
   ],
   "source": [
    "#excecute graph - test that tool calling works\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is the weather in San Francisco?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
