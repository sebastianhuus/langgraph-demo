{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "01bed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('langgraph_workflow.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "afc8f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from enum import Enum\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "class Models(Enum):\n",
    "    GEMMA3_12B_IT_QAT_Q4 = \"hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M\"\n",
    "    GEMMA3_4B = \"gemma3:4b\"\n",
    "\n",
    "    # return value of model when fetched\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "def init_ollama_chat_model(model_name: Models):\n",
    "    \"\"\"\n",
    "    Initialize the chat model from Ollama.\n",
    "    \"\"\"\n",
    "    ollama_model_name = f\"ollama:{model_name.value}\"\n",
    "\n",
    "    try:\n",
    "        model : BaseChatModel = init_chat_model(ollama_model_name)\n",
    "        logger.info(f\"Model {model} initialized successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize model {ollama_model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Class that lets me change model during runtime.\"\"\"\n",
    "    def __init__(self, initial_model: BaseChatModel):\n",
    "        self.current_model = initial_model\n",
    "\n",
    "    def switch_model(self, new_model: BaseChatModel):\n",
    "        \"\"\"Switch to a new chat model.\"\"\"\n",
    "        self.current_model = new_model\n",
    "        logger.info(f\"Switched model to {new_model}\")\n",
    "\n",
    "model_manager = ModelManager(\n",
    "    init_ollama_chat_model(\n",
    "        Models.GEMMA3_4B\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(\"Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a8d62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str):\n",
    "    \"\"\"Returns weather info for a location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather.\n",
    "\n",
    "    Example:\n",
    "        >>> get_weather(\"San Francisco\")\n",
    "    \"\"\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "def get_news():\n",
    "    \"\"\"Returns the latest news headlines.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string with the latest news headlines.\n",
    "\n",
    "    Example:\n",
    "        >>> get_news()\n",
    "    \"\"\"\n",
    "    return \"Latest news: AI is taking over the world!\"\n",
    "\n",
    "# Available tools dictionary\n",
    "TOOLS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_news\": get_news\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def create_tool_description(tools: dict):\n",
    "    \"\"\"Creates a string description of available tools that we can pass to the model.\n",
    "    \n",
    "    Includes name, description and usage information for each tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each tool, return its docstring\n",
    "    tool_descriptions = []\n",
    "    for tool_name, tool_func in tools.items():\n",
    "        signature = inspect.signature(tool_func)\n",
    "        docstring = tool_func.__doc__\n",
    "        tool_descriptions.append(f\"def {tool_name}{signature}:\\n\\\"\\\"\\\"{docstring}\\n\\\"\\\"\\\"\")\n",
    "    return \"\\n\".join(tool_descriptions)\n",
    "\n",
    "# test that it works\n",
    "logger.info(f\"Tool description created:\\n{create_tool_description(TOOLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Instructions\n",
      "You are a helpful conversational AI assistant.\n",
      "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
      "The python methods described below are imported and available, you can only use defined methods.\n",
      "ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
      "The generated code should be readable and efficient. \n",
      "\n",
      "For questions that don't require any specific tools, just respond normally without tool calls.\n",
      "\n",
      "# Instructions for using tools:\n",
      "- Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
      "- The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
      "- When using a ```tool_call``` think step by step why and how it should be used. \n",
      "- All tools will directly output a string into the `tool_output` variable. \n",
      "\n",
      "The following Python methods are available:\n",
      "\n",
      "```python\n",
      "def get_weather(location: str):\n",
      "\"\"\"Returns weather info for a location.\n",
      "\n",
      "    Args:\n",
      "        location (str): The location to get the weather for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string describing the weather.\n",
      "\n",
      "    Example:\n",
      "        >>> get_weather(\"San Francisco\")\n",
      "    \n",
      "\"\"\"\n",
      "def get_news():\n",
      "\"\"\"Returns the latest news headlines.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with the latest news headlines.\n",
      "\n",
      "    Example:\n",
      "        >>> get_news()\n",
      "    \n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "# Example usage of tools:\n",
      "You can use a tool like this:\n",
      "```tool_code\n",
      "my_tool(\"argument1\", \"argument2\")\n",
      "```\n",
      "- Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
      "\n",
      "# Bad example of tool usage:\n",
      "```tool_code\n",
      "result = my_tool(\"argument1\", \"argument2\")\n",
      "print(result)\n",
      "```\n",
      "- This code will cause an error because the tool output is not being used correctly.\n",
      "\n",
      "```tool_code\n",
      "print(my_tool(\"argument1\", \"argument2\"))\n",
      "```\n",
      "- This code will cause an error because the tool output is not being used correctly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure you do not include any \".\" in the prompt - you will get errors during the function call!\n",
    "\n",
    "def create_instruction_prompt(tool_description: str) -> str:\n",
    "    instruction_prompt = f'''\n",
    "    # Instructions\n",
    "    You are a helpful conversational AI assistant.\n",
    "    At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
    "    The python methods described below are imported and available, you can only use defined methods.\n",
    "    ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
    "    The generated code should be readable and efficient. \n",
    "\n",
    "    For questions that don't require any specific tools, just respond normally without tool calls.\n",
    "\n",
    "    # Instructions for using tools:\n",
    "    - Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
    "    - The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
    "    - When using a ```tool_call``` think step by step why and how it should be used. \n",
    "    - All tools will directly output a string into the `tool_output` variable. \n",
    "\n",
    "    The following Python methods are available:\n",
    "\n",
    "    ```python\n",
    "    {tool_description}\n",
    "    ```\n",
    "\n",
    "    # Example usage of tools:\n",
    "    You can use a tool like this:\n",
    "    ```tool_code\n",
    "    my_tool(\"argument1\", \"argument2\")\n",
    "    ```\n",
    "    - Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
    "\n",
    "    # Bad example of tool usage:\n",
    "    ```tool_code\n",
    "    result = my_tool(\"argument1\", \"argument2\")\n",
    "    print(result)\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "\n",
    "    ```tool_code\n",
    "    print(my_tool(\"argument1\", \"argument2\"))\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "    '''\n",
    "\n",
    "    return instruction_prompt\n",
    "\n",
    "print(create_instruction_prompt(create_tool_description(TOOLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9044f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(text):\n",
    "    \"\"\"Extract tool calls from model output using regex parsing.\"\"\"\n",
    "    logger.info(f\"[TOOL_PARSER] Starting tool extraction from text: {text[:500]}...\")\n",
    "    \n",
    "    pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code = match.group(1).strip()\n",
    "        logger.info(f\"[TOOL_PARSER] Found tool code: {code}\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"[TOOL_PARSER] Attempting to execute: {code}\")\n",
    "            logger.info(f\"[TOOL_PARSER] Available tools: {list(TOOLS.keys())}\")\n",
    "            \n",
    "            # Execute the tool call safely\n",
    "            result = eval(code, {\"__builtins__\": {}}, TOOLS)\n",
    "            logger.info(f\"[TOOL_PARSER] Tool execution successful: {result}\")\n",
    "            \n",
    "            return f'```tool_output\\n{result}\\n```'\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[TOOL_PARSER] Tool execution failed: {str(e)}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Error type: {type(e).__name__}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Code that failed: {code}\")\n",
    "            return f'```tool_output\\nError: {str(e)}\\n```'\n",
    "    else:\n",
    "        logger.info(\"[TOOL_PARSER] No tool_code blocks found in text\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c48d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_agent(state: MessagesState):\n",
    "    \"\"\"Single ReAct agent that can generate responses and execute tools in a loop.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    logger.info(f\"[REACT] Processing {len(messages)} messages\")\n",
    "    \n",
    "    # Always include the system prompt for tool instructions\n",
    "    system_prompt = create_instruction_prompt(create_tool_description(TOOLS))\n",
    "    \n",
    "    # Build conversation with system prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    \n",
    "    # Generate response\n",
    "    response = model_manager.current_model.invoke(conversation)\n",
    "    logger.info(f\"[REACT] Model response: {response.content[:200]}...\")\n",
    "    \n",
    "    # Check if response contains tool calls\n",
    "    if '```tool_code' in str(response.content):\n",
    "        logger.info(\"[REACT] Tool code detected - executing tools\")\n",
    "        \n",
    "        # Execute the tool call\n",
    "        tool_output = extract_tool_calls(response.content)\n",
    "        \n",
    "        if tool_output:\n",
    "            logger.info(f\"[REACT] Tool execution result: {tool_output}\")\n",
    "            \n",
    "            # Extract the result from tool_output\n",
    "            result_match = re.search(r'```tool_output\\n(.*?)\\n```', tool_output, re.DOTALL)\n",
    "            if result_match:\n",
    "                clean_result = result_match.group(1).strip()\n",
    "                logger.info(f\"[REACT] Clean result: {clean_result}\")\n",
    "                \n",
    "                # Create a new response incorporating the tool result\n",
    "                final_response_prompt = f\"\"\"Based on the tool result: {clean_result}\n",
    "                \n",
    "Please provide a helpful, natural response to the user incorporating this information. \n",
    "Do not include any tool code or technical details, just a conversational answer.\"\"\"\n",
    "                \n",
    "                # Generate final response with tool result\n",
    "                final_conversation = [\n",
    "                    {\"role\": \"system\", \"content\": final_response_prompt},\n",
    "                    {\"role\": \"user\", \"content\": messages[-1].content}\n",
    "                ]\n",
    "                \n",
    "                final_response = model_manager.current_model.invoke(final_conversation)\n",
    "                logger.info(f\"[REACT] Final response with tool result: {final_response.content}\")\n",
    "                \n",
    "                return {\"messages\": [final_response]}\n",
    "    \n",
    "    # No tool calls needed, return the response as-is\n",
    "    logger.info(\"[REACT] No tool calls detected - returning response\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue_react(state: MessagesState):\n",
    "    \"\"\"Always end after the react agent processes the input.\"\"\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "023c9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ReAct graph setup \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"react\", react_agent)\n",
    "\n",
    "builder.add_edge(START, \"react\")\n",
    "builder.add_edge(\"react\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d23aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced utils for better debugging\n",
    "def print_conversation(result):\n",
    "    print(\"=== CONVERSATION FLOW ===\")\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"\\n--- Message {i+1} ---\")\n",
    "        print(f\"Type: {type(message).__name__}\")\n",
    "        print(f\"Content: {message.content}\")\n",
    "        \n",
    "        # Check if this message contains a tool call\n",
    "        if '```tool_code' in str(message.content):\n",
    "            print(\"🔧 TOOL CALL DETECTED\")\n",
    "            \n",
    "            # Extract the tool code for debugging\n",
    "            pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                code = match.group(1).strip()\n",
    "                print(f\"📝 Tool Code: {code}\")\n",
    "                \n",
    "                # Try to execute and show result\n",
    "                tool_output = extract_tool_calls(message.content)\n",
    "                if tool_output:\n",
    "                    print(f\"🔧 Tool Result: {tool_output}\")\n",
    "                else:\n",
    "                    print(\"❌ No tool output generated\")\n",
    "            else:\n",
    "                print(\"❌ Could not extract tool code\")\n",
    "        \n",
    "        # Show if this is a tool output\n",
    "        if '```tool_output' in str(message.content):\n",
    "            print(\"📊 TOOL OUTPUT DETECTED\")\n",
    "            \n",
    "            # Extract the tool output for debugging\n",
    "            pattern = r\"```tool_output\\n(.*?)\\n```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                output = match.group(1).strip()\n",
    "                print(f\"📋 Output: {output}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f0f59c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is your name? I am barry!\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Hello Barry, my name is Tim! It’s nice to meet you.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#excecute graph - test that chat history works\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is your name? I am barry!\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f5fb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is the weather in San Francisco?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: It's 60 degrees and quite foggy here in San Francisco. A bit chilly and a lot of mist!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is the weather in San Francisco?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "444b9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What's the latest news?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Wow, it looks like the latest news is pretty dramatic – apparently AI is taking over the world! It's a big topic, and I'm sure there's a lot more to it than that headline suggests, but definitely something to keep an eye on. Do you want me to try and find some more details about that?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What's the latest news?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "54c765f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "Tool output found in message: It’s a pretty cozy day in San Francisco – it’s 60 degrees and quite foggy! You’ll definitely want a jacket on.\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It's looking pretty foggy and cool in San Francisco – it’s 60 degrees right now. You’ll definitely want a jacket!\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City today! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It’s looking pretty foggy and cool over in San Francisco – it’s 60 degrees right now. Layers are definitely your friend today!\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It’s looking pretty foggy and cool here in San Francisco – it’s 60 degrees right now. You might want a jacket!\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It's looking pretty foggy and cool in San Francisco – it's 60 degrees right now. You’ll definitely want a jacket!\n",
      "Tool output found in message: It’s a beautiful day in New York City – 90 degrees and sunny! Perfect for being outdoors.\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n",
      "==============================\n",
      "Running tests with model model='hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M'\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay hydrated and enjoy the sunshine!\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay hydrated and enjoy the sunshine!\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"What is the weather in New York City?\": [\"90 degrees\", \"sunny\"],\n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content = ai_response.content\n",
    "\n",
    "\n",
    "    if expected_outputs[0] in content and expected_outputs[1] in content:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"Tool output found in message: {content}\")\n",
    "    else:\n",
    "        print(f\"Tool output NOT found in message: {content}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)\n",
    "run_tests(Models.GEMMA3_12B_IT_QAT_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "32e09251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define more tools in attempt to overwhelm model\n",
    "def get_stock_price(symbol: str):\n",
    "    \"\"\"Returns the current stock price for a given symbol.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): The stock symbol to get the price for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the current stock price.\n",
    "\n",
    "    Example:\n",
    "        >>> get_stock_price(\"AAPL\")\n",
    "    \"\"\"\n",
    "    return f\"The current price of {symbol} is $150.00.\"\n",
    "\n",
    "def start_microwave(minutes: int):\n",
    "    \"\"\"Starts the microwave for a given number of minutes.\n",
    "    \n",
    "    Args:\n",
    "        minutes (int): The number of minutes to run the microwave.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the microwave has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_microwave(2)\n",
    "    \"\"\"\n",
    "    return f\"The microwave has been started for {minutes} minutes.\"\n",
    "\n",
    "def start_blender(speed: Literal['low', 'medium', 'high']):\n",
    "    \"\"\"Starts the blender at a given speed.\n",
    "    \n",
    "    Args:\n",
    "        speed (str): The speed to run the blender at ('low', 'medium', 'high').\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the blender has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_blender('medium')\n",
    "    \"\"\"\n",
    "    return f\"The blender has been started at {speed} speed.\"\n",
    "\n",
    "def open_front_door():\n",
    "    \"\"\"Opens the front door.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the front door has been opened or not.\n",
    "\n",
    "    Example:\n",
    "        >>> open_front_door()\n",
    "    \"\"\"\n",
    "    return \"The front door has been opened.\"\n",
    "\n",
    "TOOLS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_news\": get_news,\n",
    "    \"get_stock_price\": get_stock_price,\n",
    "    \"start_microwave\": start_microwave,\n",
    "    \"start_blender\": start_blender,\n",
    "    \"open_front_door\": open_front_door\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d4b3d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "Tool output NOT found in message: ```tool_call\n",
      "get_weather(\"front door\")\n",
      "```\n",
      "Tool output found in message: It’s looking pretty foggy and cool here in San Francisco – it’s 60 degrees right now. You’ll definitely want a jacket!\n",
      "Tool output NOT found in message: Sounds lovely out today – it’s 90 degrees and sunny! Absolutely, I’ll turn on the microwave for 2 minutes.\n",
      "Tool output NOT found in message: Okay, I’m sorry, something went wrong – I need a location to get the weather! Could you please tell me where you’d like me to check the weather for?\n",
      "Tool output NOT found in message: Okay, I understand you're trying to get the stock price of Apple (AAPL). Unfortunately, I'm having a little trouble right now – it seems like there's a problem with how I'm accessing that information. The error message indicates that I'm missing a basic command needed to get the data. \n",
      "\n",
      "Don't worry, I'm working on it! I’ll try again shortly to get you that information.\n",
      "Tool output NOT found in message: I understand you’d like me to turn on the blender on high speed. However, I’m a conversational AI and don’t have the ability to control physical devices like a blender. \n",
      "\n",
      "\n",
      "Tool output NOT found in message: I'm sorry, I can't physically open the front door. I'm a conversational AI and don't have a body!\n",
      "Tool output found in message: It’s looking pretty foggy and cool in San Francisco – it’s 60 degrees right now. You might want a jacket!\n",
      "Tool output NOT found in message: Sounds lovely out today – it’s 90 degrees and sunny! Let me get that microwave going for you.\n",
      "Tool output NOT found in message: Okay, I'm sorry, something went wrong. It looks like I'm missing a bit of information – I need to know where you want me to get the weather from! Could you tell me your location so I can get the weather for you?\n",
      "==============================\n",
      "Tests completed: 2/10 successful\n",
      "Success rate: 20.00%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"Can you open the front door?\": [\"The front door\", \"opened\"],\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"Can you turn on the microwave for 2 minutes?\": [\"The microwave\", \"started for 2 minutes\"],\n",
    "    \"Can you start the blender on medium speed?\": [\"The blender\", \"started at medium speed\"],\n",
    "    \"What is the stock price of AAPL?\": [\"The current price of AAPL\", \"$150.00\"],\n",
    "    \"Can you turn on the blender on high speed?\": [\"The blender\", \"started at high speed\"]  \n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content = ai_response.content\n",
    "\n",
    "\n",
    "    if expected_outputs[0] in content and expected_outputs[1] in content:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"Tool output found in message: {content}\")\n",
    "    else:\n",
    "        print(f\"Tool output NOT found in message: {content}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
