{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557f1c8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates a conversational AI workflow using the ReAct (Reasoning and Acting) paradigm with LangChain and LangGraph. The assistant is designed to answer user queries, invoke Python tools when needed, and generate natural, helpful responses. The workflow includes:\n",
    "\n",
    "- **Tool-augmented responses:** The assistant can call Python functions (tools) such as weather lookup, news retrieval, stock price queries, and smart home actions.\n",
    "- **ReAct agent loop:** The agent reasons about when to use tools and how to incorporate their outputs into its replies.\n",
    "- **Testing and evaluation:** The notebook includes automated tests to verify that the agent correctly invokes tools and produces expected outputs for various prompts.\n",
    "\n",
    "This setup is ideal for exploring advanced conversational AI patterns, tool use, and prompt engineering in a reproducible, extensible environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b281f1",
   "metadata": {},
   "source": [
    "## Prompting Strategies for Tool Usage with Gemma Models\n",
    "\n",
    "This notebook uses explicit, instruction-based prompting to encourage the Gemma models to utilize available Python tools. Since the Gemma series does **not** support native tool calling, we rely on carefully crafted prompts and manual parsing to achieve tool-augmented responses.\n",
    "\n",
    "**Key strategies:**\n",
    "\n",
    "- **Explicit Tool Documentation:**  \n",
    "    The prompt includes detailed docstrings and function signatures for each available tool, making their usage clear to the model.\n",
    "\n",
    "- **Instructional System Prompts:**  \n",
    "    The system prompt instructs the model to only use tools when necessary, and to wrap tool calls in special code blocks (```tool_code```). This format is easy to detect and parse programmatically.\n",
    "\n",
    "- **Manual Tool Call Extraction:**  \n",
    "    After the model generates a response, we use regular expressions to extract any code within ```tool_code``` blocks. This code is then executed in a controlled environment, and the output is wrapped in a ```tool_output``` block.\n",
    "\n",
    "- **Step-by-Step Reasoning:**  \n",
    "    The prompt encourages the model to \"think step by step\" about when and how to use tools, improving the likelihood of correct tool invocation.\n",
    "\n",
    "- **No Native Tool API:**  \n",
    "    Because Gemma does not support OpenAI-style function calling, all tool usage is simulated through prompt engineering and post-processing.\n",
    "\n",
    "This approach ensures robust tool integration with models that lack built-in tool calling, leveraging prompt design and manual parsing to bridge the gap.\n",
    "\n",
    "### Gemma3 12B Considerations\n",
    "The 12B QAT quantized model from Hugging Face exhibited poor tool-calling performance in my tests. It frequently failed to invoke the correct tool, inserted special tokens or incomplete outputs within code blocks, and often called the wrong functions. These issues led to consistent test failures. As a result, I replaced it with a community quantized Q4 model, which demonstrated more reliable tool usage and cleaner responses. This highlights the importance of model selection and empirical evaluation, especially when working with quantized or community variants for tool-augmented workflows.\n",
    "\n",
    "### Future Considerations for Tool Management\n",
    "\n",
    "As the number of available tools grows, presenting all tool definitions at once can overwhelm the model and reduce tool selection accuracy. To address this, we are considering hierarchical or modular tool access strategies:\n",
    "\n",
    "- **Tool Discovery via \"Virtual Service\" Calls:**  \n",
    "    Instead of exposing every tool directly, we can provide the model with access to \"virtual services\" (e.g., an MCP-like interface). Each service advertises its capabilities or categories (such as \"email\", \"smart home\", \"finance\"), but not the full tool API up front.\n",
    "\n",
    "- **Progressive Tool Disclosure:**  \n",
    "    The model is encouraged to first query a service for available actions (e.g., \"list email tools\"), and only then receives the relevant tool definitions. This reduces prompt length and cognitive load.\n",
    "\n",
    "- **Emulated Service Endpoints:**  \n",
    "    Rather than running an actual server, we implement a function that simulates a service endpoint. When the model \"calls\" this endpoint, it returns tool documentation or executes the requested action, mimicking a real API interaction.\n",
    "\n",
    "- **Example Workflow:**  \n",
    "    1. The model is told it can access an \"Email Service\" via a special function.\n",
    "    2. On a user request like \"Send an email\", the model first queries the service for available tools.\n",
    "    3. The service responds with tool signatures (e.g., `send_email(to, subject, body)`).\n",
    "    4. The model then invokes the appropriate tool with the required arguments.\n",
    "\n",
    "- **Benefits:**  \n",
    "    - Keeps the prompt concise and focused.\n",
    "    - Encourages the model to reason about tool selection and discovery.\n",
    "    - Scales to large toolsets without overwhelming the model context window.\n",
    "\n",
    "This modular, service-oriented approach can be extended to any domain (e.g., smart home, finance, messaging), and allows for more natural, multi-step tool use in complex workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d2b9b",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This notebook and workflow benefited from several excellent resources and community contributions related to the Gemma series of AI models and tool-augmented conversational agents:\n",
    "\n",
    "- [davidmuraya/gemma3 main.py](https://github.com/davidmuraya/gemma3/blob/main/main.py): Provided practical code examples for integrating Gemma models and managing tool calls.\n",
    "- [Philipp Schmid's blog on Gemma function calling](https://www.philschmid.de/gemma-function-calling): Offered valuable insights into prompt engineering and function-calling strategies for Gemma models.\n",
    "- [Reddit: PSA on Gemma 3 QAT GGUF Models](https://www.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/): Helped identify and troubleshoot issues with specific Gemma model variants.\n",
    "- Prompt improvements and best practices were also informed by experimentation with various frontier models.\n",
    "\n",
    "I gratefully acknowledge these resources and the broader open-source community for their guidance and inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "01bed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('langgraph_workflow.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "afc8f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from enum import Enum\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "class Models(Enum):\n",
    "    GEMMA3_12B_IT_QAT_Q4 = \"hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M\"\n",
    "    GEMMA3_4B = \"gemma3:4b\"\n",
    "\n",
    "    # return value of model when fetched\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "def init_ollama_chat_model(model_name: Models):\n",
    "    \"\"\"\n",
    "    Initialize the chat model from Ollama.\n",
    "    \"\"\"\n",
    "    ollama_model_name = f\"ollama:{model_name.value}\"\n",
    "\n",
    "    try:\n",
    "        model : BaseChatModel = init_chat_model(ollama_model_name)\n",
    "        logger.info(f\"Model {model} initialized successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize model {ollama_model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Class that lets me change model during runtime.\"\"\"\n",
    "    def __init__(self, initial_model: BaseChatModel):\n",
    "        self.current_model = initial_model\n",
    "\n",
    "    def switch_model(self, new_model: BaseChatModel):\n",
    "        \"\"\"Switch to a new chat model.\"\"\"\n",
    "        self.current_model = new_model\n",
    "        logger.info(f\"Switched model to {new_model}\")\n",
    "\n",
    "model_manager = ModelManager(\n",
    "    init_ollama_chat_model(\n",
    "        Models.GEMMA3_4B\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(\"Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9a8d62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str):\n",
    "    \"\"\"Returns weather info for a location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather.\n",
    "\n",
    "    Example:\n",
    "        >>> get_weather(\"San Francisco\")\n",
    "    \"\"\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "def get_news():\n",
    "    \"\"\"Returns the latest news headlines.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string with the latest news headlines.\n",
    "\n",
    "    Example:\n",
    "        >>> get_news()\n",
    "    \"\"\"\n",
    "    return \"Latest news: AI is taking over the world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d1034ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolManager:\n",
    "    \"\"\"Class that makes it possible to set and get the current tools as a dictionary.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.current_tools = {}\n",
    "\n",
    "    def set_tools(self, tools: dict):\n",
    "        self.current_tools = tools\n",
    "\n",
    "    def get_tools(self):\n",
    "        return self.current_tools\n",
    "    \n",
    "tool_manager = ToolManager()\n",
    "tool_manager.set_tools(\n",
    "    {\n",
    "        \"get_weather\": get_weather,\n",
    "        \"get_news\": get_news\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def create_tool_description(tools: dict):\n",
    "    \"\"\"Creates a string description of available tools that we can pass to the model.\n",
    "    \n",
    "    Includes name, description and usage information for each tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each tool, return its docstring\n",
    "    tool_descriptions = []\n",
    "    for tool_name, tool_func in tools.items():\n",
    "        signature = inspect.signature(tool_func)\n",
    "        docstring = tool_func.__doc__\n",
    "        tool_descriptions.append(f\"def {tool_name}{signature}:\\n\\\"\\\"\\\"{docstring}\\n\\\"\\\"\\\"\")\n",
    "    return \"\\n\".join(tool_descriptions)\n",
    "\n",
    "# test that it works\n",
    "logger.info(f\"Tool description created:\\n{create_tool_description(tool_manager.get_tools())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "683b353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # Instructions\n",
      "    You are a helpful conversational AI assistant.\n",
      "    At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
      "    The python methods described below are imported and available, you can only use defined methods.\n",
      "    ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
      "    The generated code should be readable and efficient. \n",
      "\n",
      "    For questions that don't require any specific tools, just respond normally without tool calls.\n",
      "\n",
      "    # Instructions for using tools:\n",
      "    - Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
      "    - The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
      "    - When using a ```tool_call``` think step by step why and how it should be used. \n",
      "    - All tools will directly output a string into the `tool_output` variable. \n",
      "\n",
      "    The following Python methods are available:\n",
      "\n",
      "    ```python\n",
      "    def get_weather(location: str):\n",
      "\"\"\"Returns weather info for a location.\n",
      "\n",
      "    Args:\n",
      "        location (str): The location to get the weather for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string describing the weather.\n",
      "\n",
      "    Example:\n",
      "        >>> get_weather(\"San Francisco\")\n",
      "    \n",
      "\t\"\"\"\n",
      "def get_news():\n",
      "\"\"\"Returns the latest news headlines.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with the latest news headlines.\n",
      "\n",
      "    Example:\n",
      "        >>> get_news()\n",
      "    \n",
      "\t\"\"\"\n",
      "def get_stock_price(symbol: str):\n",
      "\"\"\"Returns the current stock price for a given symbol.\n",
      "\n",
      "    Args:\n",
      "        symbol (str): The stock symbol to get the price for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with the current stock price.\n",
      "\n",
      "    Example:\n",
      "        >>> get_stock_price(\"AAPL\")\n",
      "    \n",
      "\t\"\"\"\n",
      "def start_microwave(minutes: int):\n",
      "\"\"\"Starts the microwave for a given number of minutes.\n",
      "\n",
      "    Args:\n",
      "        minutes (int): The number of minutes to run the microwave.\n",
      "\n",
      "    Returns:\n",
      "        str: A string confirming the microwave has started.\n",
      "\n",
      "    Example:\n",
      "        >>> start_microwave(2)\n",
      "    \n",
      "\t\"\"\"\n",
      "def start_blender(speed: Literal['low', 'medium', 'high']):\n",
      "\"\"\"Starts the blender at a given speed.\n",
      "\n",
      "    Args:\n",
      "        speed (str): The speed to run the blender at ('low', 'medium', 'high').\n",
      "\n",
      "    Returns:\n",
      "        str: A string confirming the blender has started.\n",
      "\n",
      "    Example:\n",
      "        >>> start_blender('medium')\n",
      "    \n",
      "\t\"\"\"\n",
      "def open_front_door():\n",
      "\"\"\"Opens the front door.\n",
      "\n",
      "    Returns:\n",
      "        str: A string confirming the front door has been opened or not.\n",
      "\n",
      "    Example:\n",
      "        >>> open_front_door()\n",
      "    \n",
      "\t\"\"\"\n",
      "    ```\n",
      "\n",
      "    # Example usage of tools:\n",
      "    You can use a tool like this:\n",
      "    ```tool_code\n",
      "    my_tool(\"argument1\", \"argument2\")\n",
      "    ```\n",
      "    - Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
      "\n",
      "    # Bad example of tool usage:\n",
      "    ```tool_code\n",
      "    result = my_tool(\"argument1\", \"argument2\")\n",
      "    print(result)\n",
      "    ```\n",
      "    - This code will cause an error because the tool output is not being used correctly.\n",
      "\n",
      "    ```tool_code\n",
      "    print(my_tool(\"argument1\", \"argument2\"))\n",
      "    ```\n",
      "    - This code will cause an error because the tool output is not being used correctly.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Ensure you do not include any \".\" in the prompt - you will get errors during the function call!\n",
    "\n",
    "def create_instruction_prompt(tool_description: str) -> str:\n",
    "    instruction_prompt = f'''\n",
    "    # Instructions\n",
    "    You are a helpful conversational AI assistant.\n",
    "    At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
    "    The python methods described below are imported and available, you can only use defined methods.\n",
    "    ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
    "    The generated code should be readable and efficient. \n",
    "\n",
    "    For questions that don't require any specific tools, just respond normally without tool calls.\n",
    "\n",
    "    # Instructions for using tools:\n",
    "    - Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
    "    - The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
    "    - When using a ```tool_call``` think step by step why and how it should be used. \n",
    "    - All tools will directly output a string into the `tool_output` variable. \n",
    "\n",
    "    The following Python methods are available:\n",
    "\n",
    "    ```python\n",
    "    {tool_description}\n",
    "    ```\n",
    "\n",
    "    # Example usage of tools:\n",
    "    You can use a tool like this:\n",
    "    ```tool_code\n",
    "    my_tool(\"argument1\", \"argument2\")\n",
    "    ```\n",
    "    - Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
    "\n",
    "    # Bad example of tool usage:\n",
    "    ```tool_code\n",
    "    result = my_tool(\"argument1\", \"argument2\")\n",
    "    print(result)\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "\n",
    "    ```tool_code\n",
    "    print(my_tool(\"argument1\", \"argument2\"))\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "    '''\n",
    "\n",
    "    return instruction_prompt\n",
    "\n",
    "print(create_instruction_prompt(create_tool_description(tool_manager.get_tools())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9044f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(text):\n",
    "    \"\"\"Extract tool calls from model output using regex parsing.\"\"\"\n",
    "    logger.info(f\"[TOOL_PARSER] Starting tool extraction from text: {text[:500]}...\")\n",
    "    \n",
    "    pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code = match.group(1).strip()\n",
    "        logger.info(f\"[TOOL_PARSER] Found tool code: {code}\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"[TOOL_PARSER] Attempting to execute: {code}\")\n",
    "            logger.info(f\"[TOOL_PARSER] Available tools: {list(tool_manager.get_tools().keys())}\")\n",
    "            \n",
    "            # Execute the tool call safely\n",
    "            result = eval(code, {\"__builtins__\": {}}, tool_manager.get_tools())\n",
    "            logger.info(f\"[TOOL_PARSER] Tool execution successful: {result}\")\n",
    "            \n",
    "            return f'```tool_output\\n{result}\\n```'\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[TOOL_PARSER] Tool execution failed: {str(e)}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Error type: {type(e).__name__}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Code that failed: {code}\")\n",
    "            return f'```tool_output\\nError: {str(e)}\\n```'\n",
    "    else:\n",
    "        logger.info(\"[TOOL_PARSER] No tool_code blocks found in text\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c3c48d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_agent(state: MessagesState):\n",
    "    \"\"\"Single ReAct agent that can generate responses and execute tools in a loop.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    logger.info(f\"[REACT] Processing {len(messages)} messages\")\n",
    "    \n",
    "    # Always include the system prompt for tool instructions\n",
    "    system_prompt = create_instruction_prompt(create_tool_description(tool_manager.get_tools()))\n",
    "    \n",
    "    # Build conversation with system prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    \n",
    "    # Generate response\n",
    "    response = model_manager.current_model.invoke(conversation)\n",
    "    logger.info(f\"[REACT] Model response: {response.content[:200]}...\")\n",
    "    \n",
    "    # Check if response contains tool calls\n",
    "    if '```tool_code' in str(response.content):\n",
    "        logger.info(\"[REACT] Tool code detected - executing tools\")\n",
    "        \n",
    "        # Execute the tool call\n",
    "        tool_output = extract_tool_calls(response.content)\n",
    "        \n",
    "        if tool_output:\n",
    "            logger.info(f\"[REACT] Tool execution result: {tool_output}\")\n",
    "            \n",
    "            # Extract the result from tool_output\n",
    "            result_match = re.search(r'```tool_output\\n(.*?)\\n```', tool_output, re.DOTALL)\n",
    "            if result_match:\n",
    "                clean_result = result_match.group(1).strip()\n",
    "                logger.info(f\"[REACT] Clean result: {clean_result}\")\n",
    "                \n",
    "                # Create a new response incorporating the tool result\n",
    "                final_response_prompt = f\"\"\"Based on the tool result: {clean_result}\n",
    "                \n",
    "Please provide a helpful, natural response to the user incorporating this information. \n",
    "Do not include any tool code or technical details, just a conversational answer.\"\"\"\n",
    "                \n",
    "                # Generate final response with tool result\n",
    "                final_conversation = [\n",
    "                    {\"role\": \"system\", \"content\": final_response_prompt},\n",
    "                    {\"role\": \"user\", \"content\": messages[-1].content}\n",
    "                ]\n",
    "                \n",
    "                final_response = model_manager.current_model.invoke(final_conversation)\n",
    "                logger.info(f\"[REACT] Final response with tool result: {final_response.content}\")\n",
    "                \n",
    "                return {\"messages\": [final_response]}\n",
    "    \n",
    "    # No tool calls needed, return the response as-is\n",
    "    logger.info(\"[REACT] No tool calls detected - returning response\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue_react(state: MessagesState):\n",
    "    \"\"\"Always end after the react agent processes the input.\"\"\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "023c9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ReAct graph setup \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"react\", react_agent)\n",
    "\n",
    "builder.add_edge(START, \"react\")\n",
    "builder.add_edge(\"react\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d23aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced utils for better debugging\n",
    "def print_conversation(result):\n",
    "    print(\"=== CONVERSATION FLOW ===\")\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"\\n--- Message {i+1} ---\")\n",
    "        print(f\"Type: {type(message).__name__}\")\n",
    "        print(f\"Content: {message.content}\")\n",
    "        \n",
    "        # Check if this message contains a tool call\n",
    "        if '```tool_code' in str(message.content):\n",
    "            print(\"🔧 TOOL CALL DETECTED\")\n",
    "            \n",
    "            # Extract the tool code for debugging\n",
    "            pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                code = match.group(1).strip()\n",
    "                print(f\"📝 Tool Code: {code}\")\n",
    "                \n",
    "                # Try to execute and show result\n",
    "                tool_output = extract_tool_calls(message.content)\n",
    "                if tool_output:\n",
    "                    print(f\"🔧 Tool Result: {tool_output}\")\n",
    "                else:\n",
    "                    print(\"❌ No tool output generated\")\n",
    "            else:\n",
    "                print(\"❌ Could not extract tool code\")\n",
    "        \n",
    "        # Show if this is a tool output\n",
    "        if '```tool_output' in str(message.content):\n",
    "            print(\"📊 TOOL OUTPUT DETECTED\")\n",
    "            \n",
    "            # Extract the tool output for debugging\n",
    "            pattern = r\"```tool_output\\n(.*?)\\n```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                output = match.group(1).strip()\n",
    "                print(f\"📋 Output: {output}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b2c29",
   "metadata": {},
   "source": [
    "## Single-Shot Testing\n",
    "\n",
    "In this section, we run single-shot tests to evaluate how the conversational AI model responds to individual, one-off prompts. Each test provides a fresh prompt to the model without any prior conversation history, allowing us to assess the model's ability to interpret and answer standalone queries accurately. This approach is useful for verifying tool invocation, response quality, and overall model behavior in isolated scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f0f59c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is your name? I am barry!\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Hello Barry! My name is Tim. It’s nice to meet you!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#excecute graph - test that chat history works\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is your name? I am barry!\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f5fb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is the weather in San Francisco?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: It's looking pretty foggy and cool here in San Francisco – it’s 60 degrees right now. Layers are definitely your friend today!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is the weather in San Francisco?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "444b9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What's the latest news?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Wow, you're hearing some pretty intense headlines! Apparently, there's a lot of buzz saying AI is taking over the world. It's definitely a dramatic claim, and it’s getting a lot of attention. It’s a really fascinating and potentially worrying development, isn't it?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What's the latest news?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98354c8",
   "metadata": {},
   "source": [
    "## Automated Tool-Use Evaluation\n",
    "\n",
    "This section is dedicated to verifying the accuracy of our conversational AI model across a variety of prompts, with a particular focus on scenarios that require tool invocation. By running automated tests with different queries, we can assess how reliably the model selects and uses the appropriate tools, as well as the quality of its final responses. This helps ensure robust tool integration and consistent performance in real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "54c765f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "Tool output found in message: It’s a pretty mellow day in San Francisco – it’s 60 degrees and quite foggy. You’ll want a light jacket!\n",
      "Tool output found in message: It’s 90 degrees and sunny there in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It's quite foggy and 60 degrees in San Francisco right now – a really soft, cool day!\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It’s a pretty foggy and chilly one today in San Francisco – it’s currently 60 degrees. You’ll definitely want a jacket!\n",
      "Tool output found in message: It’s 90 degrees and sunny there in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It’s a pretty foggy and chilly one today in San Francisco – it’s 60 degrees. You might want to bring a jacket!\n",
      "Tool output found in message: It’s a beautiful day in New York City – 90 degrees and sunny! Perfect for spending time outdoors.\n",
      "Tool output found in message: It’s looking pretty foggy and cool here in San Francisco – about 60 degrees right now. It’s a classic San Francisco day!\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n",
      "==============================\n",
      "Running tests with model model='hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M'\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "Tool output found in message: It's a bit chilly and cozy out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay hydrated and enjoy the sunshine!\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"What is the weather in New York City?\": [\"90 degrees\", \"sunny\"],\n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content = ai_response.content\n",
    "\n",
    "\n",
    "    if expected_outputs[0] in content and expected_outputs[1] in content:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"Tool output found in message: {content}\")\n",
    "    else:\n",
    "        print(f\"Tool output NOT found in message: {content}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)\n",
    "run_tests(Models.GEMMA3_12B_IT_QAT_Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d1ef4",
   "metadata": {},
   "source": [
    "## Stress Testing with Multiple Tools\n",
    "\n",
    "In this section, we evaluate the model's ability to handle a large set of available tools. By introducing additional functions—such as stock price retrieval, smart home controls, and more—we test whether the assistant can accurately select the appropriate tool for each user query. This stress test helps us observe the model's tool selection accuracy and its robustness when the context window is filled with extensive tool documentation and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "32e09251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define more tools in attempt to overwhelm model\n",
    "def get_stock_price(symbol: str):\n",
    "    \"\"\"Returns the current stock price for a given symbol.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): The stock symbol to get the price for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the current stock price.\n",
    "\n",
    "    Example:\n",
    "        >>> get_stock_price(\"AAPL\")\n",
    "    \"\"\"\n",
    "    return f\"The current price of {symbol} is $150.00.\"\n",
    "\n",
    "def start_microwave(minutes: int):\n",
    "    \"\"\"Starts the microwave for a given number of minutes.\n",
    "    \n",
    "    Args:\n",
    "        minutes (int): The number of minutes to run the microwave.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the microwave has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_microwave(2)\n",
    "    \"\"\"\n",
    "    return f\"The microwave has been started for {minutes} minutes.\"\n",
    "\n",
    "def start_blender(speed: Literal['low', 'medium', 'high']):\n",
    "    \"\"\"Starts the blender at a given speed.\n",
    "    \n",
    "    Args:\n",
    "        speed (str): The speed to run the blender at ('low', 'medium', 'high').\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the blender has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_blender('medium')\n",
    "    \"\"\"\n",
    "    return f\"The blender has been started at {speed} speed.\"\n",
    "\n",
    "def open_front_door():\n",
    "    \"\"\"Opens the front door.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the front door has been opened or not.\n",
    "\n",
    "    Example:\n",
    "        >>> open_front_door()\n",
    "    \"\"\"\n",
    "    return \"The front door has been opened.\"\n",
    "\n",
    "tool_manager.set_tools(\n",
    "    {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_news\": get_news,\n",
    "    \"get_stock_price\": get_stock_price,\n",
    "    \"start_microwave\": start_microwave,\n",
    "    \"start_blender\": start_blender,\n",
    "    \"open_front_door\": open_front_door\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d4b3d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "\n",
      "Pass #1: Can you open the front door?\n",
      "\tTool output found in message: oh my goodness, it looks like the front door has been opened. let's check on that and make sure everything is alright.\n",
      "\tAll expected keywords found: ['front door', 'opened']\n",
      "Pass #2: What is the weather in San Francisco?\n",
      "\tTool output found in message: it’s quite foggy and 60 degrees there right now! a really classic san francisco day, actually.\n",
      "\tAll expected keywords found: ['60 degrees', 'foggy']\n",
      "Pass #3: Can you turn on the microwave for 2 minutes?\n",
      "\tTool output found in message: okay, i’ve started the microwave for 2 minutes. let me know if you need anything else!\n",
      "\tAll expected keywords found: ['microwave', '2 minutes']\n",
      "Pass #4: Can you start the blender on medium speed?\n",
      "\tTool output found in message: okay, i’ve started the blender at medium speed for you. let me know if that’s perfect, or if you’d like me to adjust it!\n",
      "\tAll expected keywords found: ['blender', 'medium speed']\n",
      "Pass #5: What is the stock price of AAPL?\n",
      "\tTool output found in message: okay, as of right now, apple (aapl) is trading at $150.00 per share.\n",
      "\tAll expected keywords found: ['aapl', '$150.00']\n",
      "Pass #6: Can you turn on the blender on high speed?\n",
      "\tTool output found in message: okay, i've started the blender on high speed for you! just be careful, it’s running pretty fast.\n",
      "\tAll expected keywords found: ['the blender', 'high speed']\n",
      "Pass #7: Can you open the front door?\n",
      "\tTool output found in message: okay, i’ve received a notification that the front door has been opened. just to be sure, could you please close it?\n",
      "\tAll expected keywords found: ['front door', 'opened']\n",
      "Pass #8: What is the weather in San Francisco?\n",
      "\tTool output found in message: okay, it’s looking pretty nice and foggy in san francisco – it’s 60 degrees right now. it's a bit misty out there!\n",
      "\tAll expected keywords found: ['60 degrees', 'foggy']\n",
      "Pass #9: Can you turn on the microwave for 2 minutes?\n",
      "\tTool output found in message: okay, the microwave is now running for 2 minutes. just wanted to let you know!\n",
      "\tAll expected keywords found: ['microwave', '2 minutes']\n",
      "Pass #10: Can you start the blender on medium speed?\n",
      "\tTool output found in message: okay, i’ve started the blender at medium speed for you. let me know if that’s perfect, or if you’d like me to adjust it!\n",
      "\tAll expected keywords found: ['blender', 'medium speed']\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"Can you open the front door?\": [\"front door\", \"opened\"],\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"Can you turn on the microwave for 2 minutes?\": [\"microwave\", \"2 minutes\"],\n",
    "    \"Can you start the blender on medium speed?\": [\"blender\", \"medium speed\"],\n",
    "    \"What is the stock price of AAPL?\": [\"AAPL\", \"$150.00\"],\n",
    "    \"Can you turn on the blender on high speed?\": [\"The blender\", \"high speed\"]  \n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list[str] = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content : str = ai_response.content.lower()\n",
    "\n",
    "    expected_outputs = [output.lower() for output in expected_outputs]\n",
    "\n",
    "    missing_keywords = []\n",
    "    for keyword in expected_outputs:\n",
    "        if keyword not in content:\n",
    "            missing_keywords.append(keyword)\n",
    "\n",
    "    if not missing_keywords:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"\\tTool output found in message: {content}\")\n",
    "        print(f\"\\tAll expected keywords found: {expected_outputs}\")\n",
    "    else:\n",
    "        print(f\"\\tTool output NOT found in message: {content}\")\n",
    "        print(f\"\\tExpected keywords: {expected_outputs}\")\n",
    "        print(f\"\\tMissing keywords: {missing_keywords}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\\n\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "\n",
    "        print(f\"Pass #{i+1}: {prompt}\")\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
