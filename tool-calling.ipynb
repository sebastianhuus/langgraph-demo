{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557f1c8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates a conversational AI workflow using the ReAct (Reasoning and Acting) paradigm with LangChain and LangGraph. The assistant is designed to answer user queries, invoke Python tools when needed, and generate natural, helpful responses. The workflow includes:\n",
    "\n",
    "- **Tool-augmented responses:** The assistant can call Python functions (tools) such as weather lookup, news retrieval, stock price queries, and smart home actions.\n",
    "- **ReAct agent loop:** The agent reasons about when to use tools and how to incorporate their outputs into its replies.\n",
    "- **Testing and evaluation:** The notebook includes automated tests to verify that the agent correctly invokes tools and produces expected outputs for various prompts.\n",
    "\n",
    "This setup is ideal for exploring advanced conversational AI patterns, tool use, and prompt engineering in a reproducible, extensible environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "01bed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('langgraph_workflow.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "afc8f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from enum import Enum\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "class Models(Enum):\n",
    "    GEMMA3_12B_IT_QAT_Q4 = \"hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M\"\n",
    "    GEMMA3_4B = \"gemma3:4b\"\n",
    "\n",
    "    # return value of model when fetched\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "def init_ollama_chat_model(model_name: Models):\n",
    "    \"\"\"\n",
    "    Initialize the chat model from Ollama.\n",
    "    \"\"\"\n",
    "    ollama_model_name = f\"ollama:{model_name.value}\"\n",
    "\n",
    "    try:\n",
    "        model : BaseChatModel = init_chat_model(ollama_model_name)\n",
    "        logger.info(f\"Model {model} initialized successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize model {ollama_model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Class that lets me change model during runtime.\"\"\"\n",
    "    def __init__(self, initial_model: BaseChatModel):\n",
    "        self.current_model = initial_model\n",
    "\n",
    "    def switch_model(self, new_model: BaseChatModel):\n",
    "        \"\"\"Switch to a new chat model.\"\"\"\n",
    "        self.current_model = new_model\n",
    "        logger.info(f\"Switched model to {new_model}\")\n",
    "\n",
    "model_manager = ModelManager(\n",
    "    init_ollama_chat_model(\n",
    "        Models.GEMMA3_4B\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(\"Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9a8d62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str):\n",
    "    \"\"\"Returns weather info for a location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather.\n",
    "\n",
    "    Example:\n",
    "        >>> get_weather(\"San Francisco\")\n",
    "    \"\"\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "def get_news():\n",
    "    \"\"\"Returns the latest news headlines.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string with the latest news headlines.\n",
    "\n",
    "    Example:\n",
    "        >>> get_news()\n",
    "    \"\"\"\n",
    "    return \"Latest news: AI is taking over the world!\"\n",
    "\n",
    "# Available tools dictionary\n",
    "TOOLS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_news\": get_news\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4938bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def create_tool_description(tools: dict):\n",
    "    \"\"\"Creates a string description of available tools that we can pass to the model.\n",
    "    \n",
    "    Includes name, description and usage information for each tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each tool, return its docstring\n",
    "    tool_descriptions = []\n",
    "    for tool_name, tool_func in tools.items():\n",
    "        signature = inspect.signature(tool_func)\n",
    "        docstring = tool_func.__doc__\n",
    "        tool_descriptions.append(f\"def {tool_name}{signature}:\\n\\\"\\\"\\\"{docstring}\\n\\\"\\\"\\\"\")\n",
    "    return \"\\n\".join(tool_descriptions)\n",
    "\n",
    "# test that it works\n",
    "logger.info(f\"Tool description created:\\n{create_tool_description(TOOLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "683b353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # Instructions\n",
      "    You are a helpful conversational AI assistant.\n",
      "    At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
      "    The python methods described below are imported and available, you can only use defined methods.\n",
      "    ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
      "    The generated code should be readable and efficient. \n",
      "\n",
      "    For questions that don't require any specific tools, just respond normally without tool calls.\n",
      "\n",
      "    # Instructions for using tools:\n",
      "    - Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
      "    - The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
      "    - When using a ```tool_call``` think step by step why and how it should be used. \n",
      "    - All tools will directly output a string into the `tool_output` variable. \n",
      "\n",
      "    The following Python methods are available:\n",
      "\n",
      "    ```python\n",
      "    def get_weather(location: str):\n",
      "\"\"\"Returns weather info for a location.\n",
      "\n",
      "    Args:\n",
      "        location (str): The location to get the weather for.\n",
      "\n",
      "    Returns:\n",
      "        str: A string describing the weather.\n",
      "\n",
      "    Example:\n",
      "        >>> get_weather(\"San Francisco\")\n",
      "    \n",
      "\"\"\"\n",
      "def get_news():\n",
      "\"\"\"Returns the latest news headlines.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with the latest news headlines.\n",
      "\n",
      "    Example:\n",
      "        >>> get_news()\n",
      "    \n",
      "\"\"\"\n",
      "    ```\n",
      "\n",
      "    # Example usage of tools:\n",
      "    You can use a tool like this:\n",
      "    ```tool_code\n",
      "    my_tool(\"argument1\", \"argument2\")\n",
      "    ```\n",
      "    - Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
      "\n",
      "    # Bad example of tool usage:\n",
      "    ```tool_code\n",
      "    result = my_tool(\"argument1\", \"argument2\")\n",
      "    print(result)\n",
      "    ```\n",
      "    - This code will cause an error because the tool output is not being used correctly.\n",
      "\n",
      "    ```tool_code\n",
      "    print(my_tool(\"argument1\", \"argument2\"))\n",
      "    ```\n",
      "    - This code will cause an error because the tool output is not being used correctly.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Ensure you do not include any \".\" in the prompt - you will get errors during the function call!\n",
    "\n",
    "def create_instruction_prompt(tool_description: str) -> str:\n",
    "    instruction_prompt = f'''\n",
    "    # Instructions\n",
    "    You are a helpful conversational AI assistant.\n",
    "    At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```.\n",
    "    The python methods described below are imported and available, you can only use defined methods.\n",
    "    ONLY use the ```tool_code``` format when absolutely necessary to answer the user's question.\n",
    "    The generated code should be readable and efficient. \n",
    "\n",
    "    For questions that don't require any specific tools, just respond normally without tool calls.\n",
    "\n",
    "    # Instructions for using tools:\n",
    "    - Never use print statements. All tool outputs are automatically handled. Only use the tool call format as shown.\n",
    "    - The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
    "    - When using a ```tool_call``` think step by step why and how it should be used. \n",
    "    - All tools will directly output a string into the `tool_output` variable. \n",
    "\n",
    "    The following Python methods are available:\n",
    "\n",
    "    ```python\n",
    "    {tool_description}\n",
    "    ```\n",
    "\n",
    "    # Example usage of tools:\n",
    "    You can use a tool like this:\n",
    "    ```tool_code\n",
    "    my_tool(\"argument1\", \"argument2\")\n",
    "    ```\n",
    "    - Where 'my_tool' is the name of the tool you want to call, and 'argument1', 'argument2' are the arguments you want to pass to the tool.\n",
    "\n",
    "    # Bad example of tool usage:\n",
    "    ```tool_code\n",
    "    result = my_tool(\"argument1\", \"argument2\")\n",
    "    print(result)\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "\n",
    "    ```tool_code\n",
    "    print(my_tool(\"argument1\", \"argument2\"))\n",
    "    ```\n",
    "    - This code will cause an error because the tool output is not being used correctly.\n",
    "    '''\n",
    "\n",
    "    return instruction_prompt\n",
    "\n",
    "print(create_instruction_prompt(create_tool_description(TOOLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9044f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(text):\n",
    "    \"\"\"Extract tool calls from model output using regex parsing.\"\"\"\n",
    "    logger.info(f\"[TOOL_PARSER] Starting tool extraction from text: {text[:500]}...\")\n",
    "    \n",
    "    pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code = match.group(1).strip()\n",
    "        logger.info(f\"[TOOL_PARSER] Found tool code: {code}\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"[TOOL_PARSER] Attempting to execute: {code}\")\n",
    "            logger.info(f\"[TOOL_PARSER] Available tools: {list(TOOLS.keys())}\")\n",
    "            \n",
    "            # Execute the tool call safely\n",
    "            result = eval(code, {\"__builtins__\": {}}, TOOLS)\n",
    "            logger.info(f\"[TOOL_PARSER] Tool execution successful: {result}\")\n",
    "            \n",
    "            return f'```tool_output\\n{result}\\n```'\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[TOOL_PARSER] Tool execution failed: {str(e)}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Error type: {type(e).__name__}\")\n",
    "            logger.error(f\"[TOOL_PARSER] Code that failed: {code}\")\n",
    "            return f'```tool_output\\nError: {str(e)}\\n```'\n",
    "    else:\n",
    "        logger.info(\"[TOOL_PARSER] No tool_code blocks found in text\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c3c48d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_agent(state: MessagesState):\n",
    "    \"\"\"Single ReAct agent that can generate responses and execute tools in a loop.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    logger.info(f\"[REACT] Processing {len(messages)} messages\")\n",
    "    \n",
    "    # Always include the system prompt for tool instructions\n",
    "    system_prompt = create_instruction_prompt(create_tool_description(TOOLS))\n",
    "    \n",
    "    # Build conversation with system prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    \n",
    "    # Generate response\n",
    "    response = model_manager.current_model.invoke(conversation)\n",
    "    logger.info(f\"[REACT] Model response: {response.content[:200]}...\")\n",
    "    \n",
    "    # Check if response contains tool calls\n",
    "    if '```tool_code' in str(response.content):\n",
    "        logger.info(\"[REACT] Tool code detected - executing tools\")\n",
    "        \n",
    "        # Execute the tool call\n",
    "        tool_output = extract_tool_calls(response.content)\n",
    "        \n",
    "        if tool_output:\n",
    "            logger.info(f\"[REACT] Tool execution result: {tool_output}\")\n",
    "            \n",
    "            # Extract the result from tool_output\n",
    "            result_match = re.search(r'```tool_output\\n(.*?)\\n```', tool_output, re.DOTALL)\n",
    "            if result_match:\n",
    "                clean_result = result_match.group(1).strip()\n",
    "                logger.info(f\"[REACT] Clean result: {clean_result}\")\n",
    "                \n",
    "                # Create a new response incorporating the tool result\n",
    "                final_response_prompt = f\"\"\"Based on the tool result: {clean_result}\n",
    "                \n",
    "Please provide a helpful, natural response to the user incorporating this information. \n",
    "Do not include any tool code or technical details, just a conversational answer.\"\"\"\n",
    "                \n",
    "                # Generate final response with tool result\n",
    "                final_conversation = [\n",
    "                    {\"role\": \"system\", \"content\": final_response_prompt},\n",
    "                    {\"role\": \"user\", \"content\": messages[-1].content}\n",
    "                ]\n",
    "                \n",
    "                final_response = model_manager.current_model.invoke(final_conversation)\n",
    "                logger.info(f\"[REACT] Final response with tool result: {final_response.content}\")\n",
    "                \n",
    "                return {\"messages\": [final_response]}\n",
    "    \n",
    "    # No tool calls needed, return the response as-is\n",
    "    logger.info(\"[REACT] No tool calls detected - returning response\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue_react(state: MessagesState):\n",
    "    \"\"\"Always end after the react agent processes the input.\"\"\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "023c9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ReAct graph setup \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"react\", react_agent)\n",
    "\n",
    "builder.add_edge(START, \"react\")\n",
    "builder.add_edge(\"react\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d23aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced utils for better debugging\n",
    "def print_conversation(result):\n",
    "    print(\"=== CONVERSATION FLOW ===\")\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"\\n--- Message {i+1} ---\")\n",
    "        print(f\"Type: {type(message).__name__}\")\n",
    "        print(f\"Content: {message.content}\")\n",
    "        \n",
    "        # Check if this message contains a tool call\n",
    "        if '```tool_code' in str(message.content):\n",
    "            print(\"🔧 TOOL CALL DETECTED\")\n",
    "            \n",
    "            # Extract the tool code for debugging\n",
    "            pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                code = match.group(1).strip()\n",
    "                print(f\"📝 Tool Code: {code}\")\n",
    "                \n",
    "                # Try to execute and show result\n",
    "                tool_output = extract_tool_calls(message.content)\n",
    "                if tool_output:\n",
    "                    print(f\"🔧 Tool Result: {tool_output}\")\n",
    "                else:\n",
    "                    print(\"❌ No tool output generated\")\n",
    "            else:\n",
    "                print(\"❌ Could not extract tool code\")\n",
    "        \n",
    "        # Show if this is a tool output\n",
    "        if '```tool_output' in str(message.content):\n",
    "            print(\"📊 TOOL OUTPUT DETECTED\")\n",
    "            \n",
    "            # Extract the tool output for debugging\n",
    "            pattern = r\"```tool_output\\n(.*?)\\n```\"\n",
    "            match = re.search(pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                output = match.group(1).strip()\n",
    "                print(f\"📋 Output: {output}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b2c29",
   "metadata": {},
   "source": [
    "## Single-Shot Testing\n",
    "\n",
    "In this section, we run single-shot tests to evaluate how the conversational AI model responds to individual, one-off prompts. Each test provides a fresh prompt to the model without any prior conversation history, allowing us to assess the model's ability to interpret and answer standalone queries accurately. This approach is useful for verifying tool invocation, response quality, and overall model behavior in isolated scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f0f59c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is your name? I am barry!\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Hello Barry! My name is Tim. It’s nice to meet you!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#excecute graph - test that chat history works\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is your name? I am barry!\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f5fb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What is the weather in San Francisco?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: It’s a pretty typical San Francisco day – it’s 60 degrees and quite foggy! You might want to bring a jacket, it’s a bit chilly and damp out there.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What is the weather in San Francisco?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "444b9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "--- Message 1 ---\n",
      "Type: SystemMessage\n",
      "Content: You are a helpful assistant named Tim\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 2 ---\n",
      "Type: HumanMessage\n",
      "Content: What's the latest news?\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Message 3 ---\n",
      "Type: AIMessage\n",
      "Content: Wow, you won't believe this! Apparently, there's a lot of buzz saying AI is taking over the world! It sounds a little dramatic, but it’s definitely a hot topic right now. Have you heard anything about it?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified ReAct graph with weather query\n",
    "input_prompt = \"You are a helpful assistant named Tim\"\n",
    "query = \"What's the latest news?\"\n",
    "messages = [\n",
    "    SystemMessage(content=input_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "state : MessagesState = {\"messages\": messages}\n",
    "result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "print_conversation(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98354c8",
   "metadata": {},
   "source": [
    "## Automated Tool-Use Evaluation\n",
    "\n",
    "This section is dedicated to verifying the accuracy of our conversational AI model across a variety of prompts, with a particular focus on scenarios that require tool invocation. By running automated tests with different queries, we can assess how reliably the model selects and uses the appropriate tools, as well as the quality of its final responses. This helps ensure robust tool integration and consistent performance in real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "54c765f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "Tool output found in message: It’s a pretty cozy day in San Francisco – it’s 60 degrees and quite foggy! You’ll definitely want a jacket.\n",
      "Tool output found in message: It’s looking fantastic! It’s 90 degrees and sunny in New York City today – perfect for being outdoors!\n",
      "Tool output found in message: It’s quite foggy and 60 degrees there right now! A classic San Francisco day, actually. \n",
      "\n",
      "Do you want to know if that’s going to change much today?\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "Tool output found in message: It’s 60 degrees here in San Francisco and quite foggy – a classic San Francisco day! You might want to grab a jacket just in case.\n",
      "Tool output found in message: It’s a beautiful day in New York City – 90 degrees and sunny! Sounds perfect for being outdoors.\n",
      "Tool output found in message: It’s quite foggy and 60 degrees here in San Francisco today. A little chilly and a lot of mist!\n",
      "Tool output found in message: It’s a beautiful day in New York City – 90 degrees and sunny! Perfect for being outdoors.\n",
      "Tool output found in message: It's looking pretty foggy and cool in San Francisco – it’s 60 degrees right now. You’ll definitely want a jacket!\n",
      "Tool output found in message: It’s 90 degrees and sunny in New York City! Sounds like a beautiful day to be outside.\n",
      "==============================\n",
      "Tests completed: 10/10 successful\n",
      "Success rate: 100.00%\n",
      "==============================\n",
      "==============================\n",
      "Running tests with model model='hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M'\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay hydrated and enjoy the sunshine!\n",
      "Tool output found in message: It's a bit chilly and atmospheric out there! Right now, it's 60 degrees and foggy in San Francisco.\n",
      "Tool output found in message: It's 90 degrees and sunny in New York City right now! Stay cool and hydrated if you're heading out.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m* \u001b[32m30\u001b[39m)\n\u001b[32m     64\u001b[39m run_tests(Models.GEMMA3_4B)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGEMMA3_12B_IT_QAT_Q4\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mrun_tests\u001b[39m\u001b[34m(with_model)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_tests):\n\u001b[32m     53\u001b[39m     prompt = \u001b[38;5;28mlist\u001b[39m(test_prompts.keys())[i % \u001b[38;5;28mlen\u001b[39m(test_prompts)]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     result = \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     success = analyze_results(result, expected_outputs=test_prompts[prompt])\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     14\u001b[39m messages = [\n\u001b[32m     15\u001b[39m     SystemMessage(content=input_prompt),\n\u001b[32m     16\u001b[39m     HumanMessage(content=query)\n\u001b[32m     17\u001b[39m ]\n\u001b[32m     18\u001b[39m state : MessagesState = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages}\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2843\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2840\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2841\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2843\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2848\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2533\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2531\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2532\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2533\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2543\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mreact_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     10\u001b[39m conversation = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt}] + messages\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[REACT] Model response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Check if response contains tool calls\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:378\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    368\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m     **kwargs: Any,\n\u001b[32m    374\u001b[39m ) -> BaseMessage:\n\u001b[32m    375\u001b[39m     config = ensure_config(config)\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    377\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    388\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:963\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    956\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m     **kwargs: Any,\n\u001b[32m    961\u001b[39m ) -> LLMResult:\n\u001b[32m    962\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:782\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    781\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1028\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1032\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:741\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    736\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m     **kwargs: Any,\n\u001b[32m    740\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    745\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    746\u001b[39m         message=AIMessage(\n\u001b[32m    747\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    752\u001b[39m         generation_info=generation_info,\n\u001b[32m    753\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:678\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    670\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    671\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    675\u001b[39m     **kwargs: Any,\n\u001b[32m    676\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    677\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:763\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    758\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    759\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    760\u001b[39m     **kwargs: Any,\n\u001b[32m    761\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    762\u001b[39m     is_thinking = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:665\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    662\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/ollama/_client.py:165\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/langgraph-demo/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"What is the weather in New York City?\": [\"90 degrees\", \"sunny\"],\n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content = ai_response.content\n",
    "\n",
    "\n",
    "    if expected_outputs[0] in content and expected_outputs[1] in content:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"Tool output found in message: {content}\")\n",
    "    else:\n",
    "        print(f\"Tool output NOT found in message: {content}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)\n",
    "run_tests(Models.GEMMA3_12B_IT_QAT_Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d1ef4",
   "metadata": {},
   "source": [
    "## Stress Testing with Multiple Tools\n",
    "\n",
    "In this section, we evaluate the model's ability to handle a large set of available tools. By introducing additional functions—such as stock price retrieval, smart home controls, and more—we test whether the assistant can accurately select the appropriate tool for each user query. This stress test helps us observe the model's tool selection accuracy and its robustness when the context window is filled with extensive tool documentation and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "32e09251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define more tools in attempt to overwhelm model\n",
    "def get_stock_price(symbol: str):\n",
    "    \"\"\"Returns the current stock price for a given symbol.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): The stock symbol to get the price for.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the current stock price.\n",
    "\n",
    "    Example:\n",
    "        >>> get_stock_price(\"AAPL\")\n",
    "    \"\"\"\n",
    "    return f\"The current price of {symbol} is $150.00.\"\n",
    "\n",
    "def start_microwave(minutes: int):\n",
    "    \"\"\"Starts the microwave for a given number of minutes.\n",
    "    \n",
    "    Args:\n",
    "        minutes (int): The number of minutes to run the microwave.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the microwave has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_microwave(2)\n",
    "    \"\"\"\n",
    "    return f\"The microwave has been started for {minutes} minutes.\"\n",
    "\n",
    "def start_blender(speed: Literal['low', 'medium', 'high']):\n",
    "    \"\"\"Starts the blender at a given speed.\n",
    "    \n",
    "    Args:\n",
    "        speed (str): The speed to run the blender at ('low', 'medium', 'high').\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the blender has started.\n",
    "\n",
    "    Example:\n",
    "        >>> start_blender('medium')\n",
    "    \"\"\"\n",
    "    return f\"The blender has been started at {speed} speed.\"\n",
    "\n",
    "def open_front_door():\n",
    "    \"\"\"Opens the front door.\n",
    "\n",
    "    Returns:\n",
    "        str: A string confirming the front door has been opened or not.\n",
    "\n",
    "    Example:\n",
    "        >>> open_front_door()\n",
    "    \"\"\"\n",
    "    return \"The front door has been opened.\"\n",
    "\n",
    "TOOLS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_news\": get_news,\n",
    "    \"get_stock_price\": get_stock_price,\n",
    "    \"start_microwave\": start_microwave,\n",
    "    \"start_blender\": start_blender,\n",
    "    \"open_front_door\": open_front_door\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Running tests with model model='gemma3:4b'\n",
      "\n",
      "Pass #1: Can you open the front door?\n",
      "\tTool output found in message: okay, i’ve received a notification that the front door has been opened. just to be sure, can you close it please?\n",
      "\tAll expected keywords found: ['front door', 'opened']\n",
      "Pass #2: What is the weather in San Francisco?\n",
      "\tTool output found in message: it’s a pretty typical san francisco day – it’s 60 degrees and quite foggy! you’ll definitely want a jacket and maybe even a scarf today.\n",
      "\tAll expected keywords found: ['60 degrees', 'foggy']\n",
      "Pass #3: Can you turn on the microwave for 2 minutes?\n",
      "\tTool output found in message: okay, the microwave has been started for 2 minutes. just wanted to let you know it’s running!\n",
      "\tAll expected keywords found: ['microwave', '2 minutes']\n",
      "Pass #4: Can you start the blender on medium speed?\n",
      "\tTool output NOT found in message: okay, i’ve started the blender on medium speed for you. let me know if that’s perfect!\n",
      "\tExpected keywords: ['blender', 'started at medium speed']\n",
      "\tMissing keywords: ['started at medium speed']\n",
      "Pass #5: What is the stock price of AAPL?\n",
      "\tTool output NOT found in message: okay, as of right now, apple (aapl) is trading at $150.00 per share.\n",
      "\tExpected keywords: ['the current price of aapl', '$150.00']\n",
      "\tMissing keywords: ['the current price of aapl']\n",
      "Pass #6: Can you turn on the blender on high speed?\n",
      "\tTool output NOT found in message: okay, i’ve started the blender on high speed for you. let me know if that’s perfect!\n",
      "\tExpected keywords: ['the blender', 'started at high speed']\n",
      "\tMissing keywords: ['started at high speed']\n",
      "Pass #7: Can you open the front door?\n",
      "\tTool output found in message: oh dear, it looks like the front door has been opened. let’s check on that.\n",
      "\tAll expected keywords found: ['front door', 'opened']\n",
      "Pass #8: What is the weather in San Francisco?\n",
      "\tTool output found in message: it’s looking pretty foggy and chilly here in san francisco – it’s 60 degrees right now. layers are definitely recommended!\n",
      "\tAll expected keywords found: ['60 degrees', 'foggy']\n",
      "Pass #9: Can you turn on the microwave for 2 minutes?\n",
      "\tTool output found in message: okay, the microwave is now running for 2 minutes. let me know if you need anything else!\n",
      "\tAll expected keywords found: ['microwave', '2 minutes']\n",
      "Pass #10: Can you start the blender on medium speed?\n",
      "\tTool output NOT found in message: okay, i’ve started the blender on medium speed for you!\n",
      "\tExpected keywords: ['blender', 'started at medium speed']\n",
      "\tMissing keywords: ['started at medium speed']\n",
      "==============================\n",
      "Tests completed: 6/10 successful\n",
      "Success rate: 60.00%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Disable logging for this cell   \n",
    "#import logging                    \n",
    "#logging.disable(logging.CRITICAL) \n",
    "\n",
    "test_prompts = {\n",
    "    \"Can you open the front door?\": [\"front door\", \"opened\"],\n",
    "    \"What is the weather in San Francisco?\": [\"60 degrees\", \"foggy\"],\n",
    "    \"Can you turn on the microwave for 2 minutes?\": [\"microwave\", \"2 minutes\"],\n",
    "    \"Can you start the blender on medium speed?\": [\"blender\", \"started\", \"medium speed\"],\n",
    "    \"What is the stock price of AAPL?\": [\"AAPL\", \"$150.00\"],\n",
    "    \"Can you turn on the blender on high speed?\": [\"The blender\", \"started\", \"high speed\"]  \n",
    "}\n",
    "\n",
    "def test(prompt: str = \"What is the weather in San Francisco?\"):\n",
    "    # Test the simplified ReAct graph with weather query\n",
    "    input_prompt = \"You are a helpful assistant named Tim\"\n",
    "    query = prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=input_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    state : MessagesState = {\"messages\": messages}\n",
    "    result = graph.invoke({\"messages\": state[\"messages\"]})\n",
    "    return result\n",
    "\n",
    "def analyze_results(result, expected_outputs: list[str] = [\"60 degrees\", \"foggy\"]):\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # verify llm output a correct answer\n",
    "    llm_output_is_correct = False\n",
    "\n",
    "    ai_response = messages[-1]\n",
    "    content : str = ai_response.content.lower()\n",
    "\n",
    "    expected_outputs = [output.lower() for output in expected_outputs]\n",
    "\n",
    "    missing_keywords = []\n",
    "    for keyword in expected_outputs:\n",
    "        if keyword not in content:\n",
    "            missing_keywords.append(keyword)\n",
    "\n",
    "    if not missing_keywords:\n",
    "        llm_output_is_correct = True\n",
    "        print(f\"\\tTool output found in message: {content}\")\n",
    "        print(f\"\\tAll expected keywords found: {expected_outputs}\")\n",
    "    else:\n",
    "        print(f\"\\tTool output NOT found in message: {content}\")\n",
    "        print(f\"\\tExpected keywords: {expected_outputs}\")\n",
    "        print(f\"\\tMissing keywords: {missing_keywords}\")\n",
    "\n",
    "    if llm_output_is_correct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def run_tests(with_model: Models):\n",
    "    success_count = 0\n",
    "    total_tests = 10\n",
    "\n",
    "    model_manager.switch_model(init_ollama_chat_model(with_model))\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Running tests with model {model_manager.current_model}\\n\")\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        prompt = list(test_prompts.keys())[i % len(test_prompts)]\n",
    "        result = test(prompt)\n",
    "\n",
    "        print(f\"Pass #{i+1}: {prompt}\")\n",
    "        success = analyze_results(result, expected_outputs=test_prompts[prompt])\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "    print(\"=\"* 30)\n",
    "    print(f\"Tests completed: {success_count}/{total_tests} successful\")\n",
    "    print(\"Success rate: {:.2f}%\".format((success_count / total_tests) * 100))\n",
    "    print(\"=\"* 30)\n",
    "\n",
    "run_tests(Models.GEMMA3_4B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
